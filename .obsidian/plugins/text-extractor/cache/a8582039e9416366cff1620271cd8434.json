{"path":"Aprendizaje Autom√°tico/pdfs/casos.pdf","text":"APRENDIZAJE BASADO EN CASOS Aprendizaje Autom√°tico - InCo √çndice ‚Ä¢ Introducci√≥n ‚Ä¢ K-Nearest Neighbor ‚Ä¢ Regresi√≥n local ponderada ‚Ä¢ Razonamiento basado en casos ‚Ä¢ Algoritmos perezosos vs. algoritmos ansiosos 2Aprendizaje Autom√°tico - InCo Introducci√≥n ‚Ä¢ La idea es crear un clasificador ‚Äòperezoso‚Äô. ‚Ä¢ Para clasificar una nueva instancia, utilizo aquellas que m√°s se le parecen de las que ya conozco. ‚Ä¢ Ventajas: ‚Ä£ Para cada nueva instancia puedo obtener un clasificador diferente. ‚Ä£ La descripci√≥n de las instancia puede ser tan compleja como quiera. ‚Ä¢ Desventajas: ‚Ä£ El costo de clasificaci√≥n puede ser alto. ‚Ä£ Atributos irrelevantes pueden afectar la medida de similitud. 3Aprendizaje Autom√°tico - InCo K - Nearest Neighbor ‚Ä¢ Queremos aproximar un concepto, utilizando las k instancias m√°s cercanas a un elemento <a 1 ,...,a n > que deseamos clasificar. ‚Ä¢ Utilizamos la funci√≥n de distancia euclidiana: ‚Ä¢ El valor que damos para x ? es: ‚Ä¢ Si la funci√≥n es real, se puede utilizar: 4 + + + + - - - - - x ? d( < a 1 , ‚Ä¶, a n > , < b 1 , . . . , b n > ) = ‚àë (a i ‚àí b i ) 2 h(x ? ) ‚Üê argmax v‚àà+,‚àí ‚àë x i ‚ààk‚àínn(x ? ) Œ¥(v, f (x i )) h(x ? ) ‚Üê 1 k ‚àë x i ‚ààk‚àínn(x ? ) f(x i ) Aprendizaje Autom√°tico - InCo K - Nearest Neighbor ‚Ä¢ Los ejemplos m√°s cercanos a la instancia x ? podr√≠an considerarse m√°s importantes que los lejanos... en donde: ‚Ä¢ Si ponderamos por la distancia, podemos usar a todo el conjunto para evaluar una nueva instancia. [M√©todo de Shepard] 5 h dis (x ? ) ‚Üê argma x v‚àà{+,‚àí} ‚àë xi‚ààk‚àínn(x ? ) w i . Œ¥(v, f (x i )) h real (x ? ) ‚Üê ‚àë x i ‚ààk‚àínn(x ? ) w i . f (x i ) ‚àë w i w i = 1 d(x i , x ? ) 2 Aprendizaje Autom√°tico - InCo K - Nearest Neighbor ‚Ä¢ Al tomar en cuenta a varios vecinos, el algoritmo es menos sensible al ruido. ‚Ä¢ Pero ¬øcu√°ntos vecinos elegir? ‚Ä£ Si se elige k muy bajo, el resultado es muy sensible al ruido; si es muy alto, las zonas que tengan muchos ejemplos pueden acaparar a zonas que tengan menos. ‚Ä£ Por lo general se elige un k impar para no tener problemas de empate. Los valores usuales son bajos: 1, 3 y 5. ‚Ä£ Una forma de estimar k es probando distintos valores, midiendo la performance dejando un elemento del conjunto afuera y clasificando con el resto (1-out-cross-validation). 6Aprendizaje Autom√°tico - InCo K - Nearest Neighbor ‚Ä¢ A diferencia de los √°rboles de decisi√≥n, al aplicar la distancia euclidiana, se tienen en cuenta TODAS las dimensiones que describen un atributo ‚Ä¢ Problemas: ‚Ä£ si 2 atributos en 20 son los relevantes, instancias que en realidad son muy diferentes pueden estar muy pr√≥ximas en el espacio (maldici√≥n de las dimensiones). ‚Ä£ ¬øQu√© sucede con atributos con distintas magnitudes? 7Aprendizaje Autom√°tico - InCo K - Nearest Neighbor ‚Ä¢ Maldici√≥n de las dimensiones: ‚Ä£ Una posible soluci√≥n es asignar pesos a los distintos atributos. Esto corresponde a modificar el largo de los ejes del espacio. ‚Ä£ ¬øC√≥mo se determina cu√°nto hay que acortar o alargar un eje? ‡πè Penalizando los atributos cuyos distribuci√≥n es uniforme en cada clase. ‡πè Utilizando validaci√≥n cruzada. 8Aprendizaje Autom√°tico - InCo K - Nearest Neighbor ‚Ä¢ ¬øQu√© sucede con atributos con distintas magnitudes? ‚Ä£ Estandarizaci√≥n: se asume que A es ùí©(ùúá A ,ùúé A ) y se lleva a ùí©(0,1)‚Ä® (z score) ‚Ä£ Reescalamiento: se cambia el rango de los atributos‚Ä® ‚Ä® ‚Ä£ OneHot: se transforma atributos categoriales en vectores 9 Est(x, A) = x ‚àí Œº A œÉ A Resc(x, A) = x ‚àí min A max A ‚àí min A RescAbs(x, A) = x max A ‚àí min A OneHot(x 2 , {a 1 , a 2 , ‚Ä¶, a n }) = (0,1,‚Ä¶,0) Aprendizaje Autom√°tico - InCo K - Nearest Neighbor ‚Ä¢ Cada clasificaci√≥n implica la selecci√≥n de k ejemplos; esto puede ser muy costoso si contamos con un gran conjunto de ejemplos. ‚Ä¢ Esto se soluciona implementando una buena indexaci√≥n sobre el conjunto. ‚Ä¢ ¬øCu√°l es el sesgo inductivo de K-NN? La clasificaci√≥n de una instancia es parecida a las de sus k vecinos (cercan√≠a implica similitud). 10Aprendizaje Autom√°tico - InCo Regresi√≥n local ponderada ‚Ä¢ Generalizamos KNN, creando una aproximaci√≥n local de la funci√≥n objetivo y construimos una funci√≥n h que aproxime a los puntos cercanos a x ? ‚Ä¢ Podemos elegir qu√© modelo de funci√≥n utilizamos para esta aproximaci√≥n (lineal, cuadr√°tica, etc.). ‚Ä¢ Por ejemplo: ‚Ä£ Buscamos al vector que minimice el error de la funci√≥n sobre los k puntos cercanos. ‚Ä£ Pero, ¬øqu√© funci√≥n de error deber√≠amos considerar? 11 h( < a 1 , ‚Ä¶, a n > ) = w 0 + w 1 . a 1 + ‚Ä¶ + w n . a n (w 0 , w 1 , ‚Ä¶, w n ) Aprendizaje Autom√°tico - InCo Regresi√≥n local ponderada ‚Ä¢ Algunas opciones: ‚Ä£ Sobre los k vecinos cercanos: ‚Ä£ Ponderando a todo el conjunto: ‚Ä£ Ponderando a los k-cercanos: Donde K es una funci√≥n decreciente. 12 E = 1 2 ‚àë x‚ààk‚àínear(x ? ) ( f(x) ‚àí h(x)) 2 E = 1 2 ‚àë x‚ààD ( f (x) ‚àí h(x)) 2 K(d(x, x ? )) E = 1 2 ‚àë x‚ààk‚àínear(x ? ) ( f (x) ‚àí h(x)) 2 K(d(x, x ? )) Aprendizaje Autom√°tico - InCo Regresi√≥n local ponderada ‚Ä¢ En la selecci√≥n de la funci√≥n de error se debe considerar cu√°nto queremos que influya el total del conjunto de entrenamiento vs. el costo computacional. ‚Ä¢ Debido al costo en la reducci√≥n del error, se utilizan por lo general funciones lineales o cuadr√°ticas. ‚Ä¢ Luego de clasificar la nueva instancia, podemos descartar a la hip√≥tesis encontrada: cada instancia genera una nueva aproximaci√≥n. 13Aprendizaje Autom√°tico - InCo Razonamiento basado en casos ‚Ä¢ ¬øQu√© sucede cuando las instancias son representadas de forma m√°s compleja? ‚Ä¢ Al igual que KNN y RLP, clasificamos una instancia en base a casos parecidos: en lugar de puntos en un espacio eucl√≠deo, las instancias se representan con atributos m√°s complejos. ‚Ä¢ Se debe buscar una m√©trica de similitud que depende del dominio de trabajo. ‚Ä¢ La soluci√≥n se basa en combinaciones complejas y espec√≠ficas al dominio de aplicaci√≥n. 14 Satisfacci√≥n: ???? Transporte: <√≥mnibus> Tiempo: <dia +1 : nublado, dia +2 : soleado, dia +3 :?> Lugar: <casa[cuartos: 2, piscina: no), centro:10 km> Personas:<adultos(hombres: 1, mujeres: 1), ni√±os=4> } Aprendizaje Autom√°tico - InCo Algoritmos perezosos vs. ansiosos ‚Ä¢ Los algoritmos que vimos (KNN, Regresi√≥n Local‚Ä¶) difieren el c√°lculo de una hip√≥tesis hasta la llegada de una nueva consulta (algoritmos perezosos). ‚Ä¢ Estos algoritmos computan una aproximaci√≥n local de la funci√≥n objetivo para responder cada nueva consulta: utilizan m√∫ltiples aproximaciones locales para modelar la funci√≥n objetivo [global]. ‚Ä¢ Los algoritmos ansiosos pueden utilizar tambi√©n aproximaciones; sin embargo, √©stas quedan ¬´fijas¬ª al conjunto de entrenamiento. 15Aprendizaje Autom√°tico - InCo Algoritmos perezosos vs. ansiosos ‚Ä¢ Dado un mismo espacio de hip√≥tesis, los algoritmos perezosos tienen una mayor poder de adaptaci√≥n a una nueva consulta. ‚Ä¢ El costo de clasificaci√≥n, sin embargo, aumenta: la aproximaci√≥n se realiza en tiempo de clasificaci√≥n y no de entrenamiento. ‚Ä¢ Se precisan estructuras eficientes para determinar los ejemplos cercanos a la instancia a clasificar. 16","libVersion":"0.3.1","langs":""}