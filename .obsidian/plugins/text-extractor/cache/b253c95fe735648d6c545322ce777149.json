{"path":"Archivados/Aprendizaje Automático/pdfs/conceptual.pdf","text":"APRENDIZAJE CONCEPTUAL Aprendizaje Automático - InCo Índice • Definición • Aprendizaje conceptual como búsqueda • Algoritmo FIND-S • Algoritmo CANDIDATE-ELIMINATION • Sesgo Inductivo 2Aprendizaje Automático - InCo Definición • Aprendizaje Conceptual: Inferir un concepto —una función booleana— a partir de un conjunto de entrenamiento. • Ejemplo: ¿Cuándo salva Pedro un examen? • Se define concepto objetivo c: c: X →{0,1} en donde X es el conjunto de instancias. • En el ejemplo, c(x) = 1 implica que Pedro salva el examen para la instancia 3 x ∈ X Aprendizaje Automático - InCo Definición • Supongamos que contamos con el siguiente conjunto de entrenamiento: ➡ Alguien ya eligió por mí los atributos para representar las instancias en este problema. 4 #Ej x c(x) Dedicación Dificultad Horario Humedad Humor Doc Salva 1 Alta Alta Nocturno Media Bueno SÍ 2 Baja Media Matutino Alta Malo NO 3 Media Alta Nocturno Media Malo SÍ 4 Media Alta Matutino Alta Bueno NO Aprendizaje Automático - InCo Definición • Puedo elegir cómo representar al concepto buscado. • A los posibles candidatos los denominamos hipótesis. • Para fijar ideas utilizaremos un espacio H con hipótesis de la forma: h:<Dedicación, Dificultad, Horario, Humedad, HumorDoc> ‣ h representa una conjunción de restricciones sobre cada atributo ‣ Para cada atributo, se tienen los posibles valores: ๏ Valor específico: alto, medio,... ๏ Todo valor es aceptable: ? ๏ Ningún valor es aceptable: ∅ 5Aprendizaje Automático - InCo Definición • La hipótesis “Pedro salva un examen estudia mucho”, se puede representar por: h: <Alta, ?, ?, ?, ?> • Hipótesis más general, “Pedro salva siempre”: <?, ?, ?, ?, ?> • Hipótesis más específica, “Pedro siempre pierde”: <∅, ∅, ∅, ∅, ∅>, pero también <Alta, ∅, ∅, ∅, ∅>, <∅, ?, ∅, Media, Bueno>, … ➡ Estos ejemplos son válidos para el espacio H elegido. ¿Qué sucede si h es una disyunción de restricciones en lugar de una conjunción? 6Aprendizaje Automático - InCo Definición • Con el conjunto de entrenamiento dado... ¿<Alta, ?, ?, ?, ?> es una buena hipótesis para ser el concepto objetivo? • ¿Esta es una buena elección para representar a las hipótesis si el concepto objetivo es “Pedro salva cuando su dedicación es alta o la dificultad es baja”? 7Aprendizaje Automático - InCo Definición • Dados: ‣ Dominio de instancias X ‣ Función objetivo: c: X →{0,1} ‣ Espacio de hipótesis H: H={h 0 , h 1 ,…}, h i : X→{0,1} ‣ Conjunto de Entrenamiento D: D={ [x 0 ,c(x 0 )], …, [x n ,c(x n )] / x ∈ X} • Objetivo: Determinar una hipótesis h∈H tal que h(x)=c(x) para todo x∈X. 8Aprendizaje Automático - InCo Definición • En nuestro ejemplo: ‣ X es el conjunto de instancias formadas por los atributos: ๏ Dedicación: alta, media, baja. ๏ Dificultad: alta, media, baja. ๏ Horario: matutino, nocturno. ๏ Humedad: alta, media, baja. ๏ HumorDoc: bueno, malo. ‣ c: X →{0,1} / c(x)=1 Pedro salva bajo las condiciones x. ‣ H = {h / h es la conjunción de restricciones sobre los atributos, tomando un valor específico, cualquier valor o ninguno } ‣ D está dado por la tabla de la transparencia 4. 9Aprendizaje Automático - InCo Definición • La única información que tenemos de c son los valores en el conjunto de entrenamiento. • Nada nos garantiza que el concepto objetivo pertenezca al conjunto H que elegimos. • Hipótesis de Aprendizaje Inductivo: Toda hipótesis que aproxime correctamente el concepto objetivo en un conjunto de ejemplos lo suficientemente grande también lo hará sobre las instancias aún no observadas. 10Aprendizaje Automático - InCo Aprendizaje = Búsqueda • Nuestro problema es encontrar una hipótesis en el espacio elegido • En nuestro ejemplo, tenemos 2000 [5*5*4*5*4] hipótesis sintácticamente distintas • ¿Cuántas semánticamente distintas hay?  • Tenemos que desarrollar estrategias para buscar en espacios muy grandes o infinitos. 11Aprendizaje Automático - InCo Aprendizaje = Búsqueda 12 • Se necesitan estrategias para buscar en el espacio de hipótesis sin tener que listarlas todas. • Definiciones: ‣ Más general o igual: Sean h j y h k funciones booleanas sobre X h j ≥ h k sii (∀x ∈ X) h k (x)=1→h j (x)=1 ‣ Análogamente, definimos más específica o igual: h j ≤ h k sii (∀x ∈ X) h j (x)=1→h k (x)=1 o h k (x)= 0→h j (x)= 0 Aprendizaje Automático - InCo * * * * * * * * * * * H h j h i h k Aprendizaje = Búsqueda 13 x x x x x x x x h k h i h j X pero no se cumple: h j ≥ h i h i ≥ h j h k ≥ h i h k ≥ h j se cumple: Aprendizaje Automático - InCo Recordamos el ejemplo 14 #Ej x c(x) Dedicación Dificultad Horario Humedad Humor Doc Salva 1 Alta Alta Nocturno Media Bueno SÍ 2 Baja Media Matutino Alta Malo NO 3 Media Alta Nocturno Media Malo SÍ 4 Media Alta Matutino Alta Bueno NO Aprendizaje Automático - InCo Find-S • ¿Cómo realizar la búsqueda de c en H? • Algoritmo FIND-S: empiezo con la hipótesis más específica y a medida que tengo ejemplos generalizo. • En nuestro ejemplo: <∅, ∅, ∅, ∅, ∅> + [x 1 , sí] <Alta, Alta, Nocturno, Media, Bueno> + [x 3 , sí] <?, Alta, Nocturno, Media, ?> ➡ Esta forma de generalizar es válida para el H del ejemplo, pero no necesariamente para otro. 15Aprendizaje Automático - InCo Find-S • Algoritmo FIND-S: h ≡ hipótesis más específica de H Para cada instancia positiva x Para cada restricción r de h Si x no satisface r, sustituir r por una restricción más general que satisfaga x. Devolver h • ¿Qué sucede con los ejemplos negativos? • Notar que en el H de nuestro ejemplo siempre tenemos una única hipótesis más específica, pero en principio podrían ser varias. 16Aprendizaje Automático - InCo Find-S • Llegamos a una hipótesis válida, pero… ‣ ¿Llegamos al concepto correcto? ‣ ¿Por qué preferir la hipótesis más específica? ‣ ¿Qué pasa si hay varias hipótesis más específicas? ¿Por qué quedarnos con sólo una de ellas? ‣ ¿Y si en el conjunto de entrenamiento hay ejemplos mal clasificados? 17Aprendizaje Automático - InCo Candidate-Elimination • Definiciones: ‣ Una hipótesis h es consistente con un conjunto de entrenamiento: Consistente(h, D) ≡ ∀[x,c(x)] ∈ D, h(x)=c(x). ‣ Espacio de versiones VS H,D VS H,D = {h ∈ H / consistente(h,D)}. • El espacio de versiones representa a todas las hipótesis candidatas a ser el objetivo buscado, dado el conjunto de entrenamiento. • ¿Cómo calcular el espacio de versiones? 18Aprendizaje Automático - InCo Candidate-Elimination • Algoritmo List-Then-Eliminate: VS ≡ conjunto con TODAS las hipótesis. Para cada ejemplo [x, c(x)]: Eliminar todas las h tq. h(x)≠c(x). Devolver VS. ‣ Enumera todo el espacio de hipótesis. ‣ En espacios H infinitos… 19Aprendizaje Automático - InCo Candidate-Elimination • Representemos el espacio de versiones con las hipótesis consistentes más específicas y más generales. • Límite general G H,D : G H,D ≡ {g ∈ H / consistente(g,D) y (¬∃g’∈H) g’>g ∧ consistente(g’,D)} • Límite específico S H,D : S H,D ≡ {s ∈ H / consistente(s,D) y (¬∃s’∈H) s>s’ ∧ consistente(s’,D)} • Teorema de la representación del espacio de versiones VS H,D = {h ∈ H / (∃ s∈S) (∃g∈G) g ≥ h ≥ s} ‣ En otras palabras, con G H,D y S H,D puedo representar a todo VS H,D . 20Aprendizaje Automático - InCo Candidate-Elimination • En nuestro ejemplo: 21 <?, ?, ?, Media, ?> <?, ?, Nocturno, ?, ?> <?, Alta, ?, Media, ?> <?, ?, Nocturno, Media, ?> <?, Alta, Nocturno, ?, ?> <?, Alta, Nocturno, Media, ?> G S Aprendizaje Automático - InCo Candidate-Elimination • Algoritmo Candidate-Elimination: S ≡ conjunto de hipótesis más específicas. G ≡ conjunto de hipótesis más generales. Para cada instancia x. Si x es un ejemplo positivo. Remover de G cualquier hipótesis inconsistente con x. Para cada hipótesis s de S, inconsistente con x. Cambiarla por todas las generalizaciones mínimas de s, consistentes con x, para las cuales haya una hipótesis en G más general que ellas. Remover de S toda hipótesis más general que otra hipótesis de S. 22Aprendizaje Automático - InCo Candidate-Elimination • Algoritmo Candidate-Elimination (cont): ... Si x es un ejemplo negativo. Remover de S cualquier hipótesis inconsistente con x. Para cada hipótesis g de G, inconsistente con x. Cambiarla por todas las especializaciones mínimas de g, consistentes con x, para las cuales haya una hipótesis en S más específica que ellas. Remover de G toda hipótesis más específica que otra hipótesis de G. 23Aprendizaje Automático - InCo Candidate-Elimination • Observar que: ‣ El cálculo de S es una generalización de FIND-S. ‣ Las operaciones de generalización y especialización dependen del espacio de trabajo H elegido. 24Aprendizaje Automático - InCo Ejemplo Candidate-Elimination 25 Supongamos G H,D = {h a , h b , h c } S H,D = {h i, h j , h k } Llega un ejemplo x, tal que: • c(x) = False • h j (x) = h a (x) = h b (x) = True • h i (x) = h k (x) = h c (x) = False • h 1 (x) = h 2 (x) = False • h 3 (x) = ? * * * * * * H h i h k h j * * * * * h a h b h c h 1 h 2 h 4 h 5 1. ¿Qué hipótesis forman VS H,D ? 2. ¿Cómo queda VS H,D U {<x,False>} ? h 3 Aprendizaje Automático - InCo Volviendo a Pedro… 26 #Ej x c(x) Dedicación Dificultad Horario Humedad Humor Doc Salva 1 Alta Alta Nocturno Media Bueno SÍ 2 Baja Media Matutino Alta Malo NO 3 Media Alta Nocturno Media Malo SÍ 4 Media Alta Matutino Alta Bueno NO Aprendizaje Automático - InCo Candidate-Elimination • En nuestro ejemplo: S0={<∅,∅,∅,∅,∅>} G0={<?,?,?,?,?>} S1={<Alta,Alta,Nocturno,Media,Bueno>} G1={<?,?,?,?,?>} S2={<Alta,Alta,Nocturno,Media,Bueno>} G2={<Alta,?,?,?,?>, <?,Alta,?,?,?>, <?,?,Nocturno,?,?>, <?,?,?,Media,?>, <?,?,?,?,Bueno>} S3={<?,Alta,Nocturno,Media,?>} G3={<?,Alta,?,?,?>, <?,?,Nocturno,?,?>, <?,?,?,Media,?>} S4={<?,Alta,Nocturno,Media,?>} G4={<?,?,Nocturno,?,?>, <?,?,?,Media,?>} • ¿qué pasa si procesamos los ejemplos en otro orden? 27Aprendizaje Automático - InCo • ¿El algoritmo converge a una hipótesis correcta? La respuesta es afirmativa cuando: ‣ Los datos de entrenamiento no contienen ruido (errores). ‣ Hay una hipótesis en H que describe el concepto objetivo. • Si tuviésemos un oráculo, ¿cuál ejemplo nos conviene elegir como siguiente a procesar? Candidate-Elimination 28Aprendizaje Automático - InCo Candidate-Elimination • ¿Cuál ejemplo elegiríamos para procesar? ‣ El mejor ejemplo es aquel que hace que la mitad del espacio diga ‘sí’, y la otra mitad diga ‘no’. ‣ Esto reduce el tamaño del espacio de versiones a la mitad. 29Aprendizaje Automático - InCo Candidate-Elimination • Podemos utilizar conocimientos parciales: ‣ Si una instancia satisface a toda hipótesis de S, con seguridad debe ser clasificada positiva. ‣ Si una instancia no satisface a ninguna de las hipótesis de G, con seguridad debe ser clasificada negativa. 30Aprendizaje Automático - InCo Candidate-Elimination • Por ejemplo, ¿cómo se clasifica a...? ๏ [Alta, Alta, Nocturno, Media, Malo] ๏ [Alta, Baja, Matutino, Alta, Bueno] ๏ [Alta, Baja, Nocturno, Media, Bueno] 31 <?, ?, ?, Media, ?> <?, ?, Nocturno, ?, ?> <?, Alta, ?, Media, ?> <?, ?, Nocturno, Media, ?> <?, Alta, Nocturno, ?, ?> <?, Alta, Nocturno, Media, ?> G S Aprendizaje Automático - InCo Sesgo Inductivo • ¿Qué sucede si el concepto objetivo no está considerado en H? • Por ejemplo, este concepto no está en el H del ejemplo: <Alta,?,?,?,?> ∨ <?,Baja,?,?,?> • Podríamos considerar el espacio con TODOS los conceptos posibles, pero… ¿cómo quedaría VS H,D ? • Si no se hacen suposiciones previas sobre la forma del concepto que se busca.... ¡nada se puede aprender! 32Aprendizaje Automático - InCo Sesgo Inductivo • Sesgo Inductivo del algoritmo L es el conjunto mínimo de suposiciones B tales que: (∀x∈X) [(B ∧ D ∧ x)→ L clasifica correctamente a x] • ¿Cuál es el sesgo inductivo de Find-S y Candidate–Elimination? 33","libVersion":"0.3.1","langs":""}