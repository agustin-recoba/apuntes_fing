{"path":"Aprendizaje Automático/pdfs/refuerzos.pdf","text":"APRENDIZAJE POR REFUERZO Métodos de Aprendizaje Automático - InCo Índice • Definición • Aprendizaje Q • Ambiente no-determinista • Diferencia Temporal • Representación / Generalización 2Métodos de Aprendizaje Automático - InCo Definición • La tarea consiste en aprender una estrategia de comportamiento para un agente de forma de realizar un objetivo dado 3 s a r Agente Estado t Estado t+1 • Se considera que: ‣ El agente se encuentra en un estado s, que puede sensar de alguna forma. ‣ El agente puede elegir realizar una acción a en ese estado. ‣ Su acción provoca un cambio en el mundo (un nuevo estado). ‣ El comportamiento es calificado con una recompensa r. Métodos de Aprendizaje Automático - InCo Definición 4Métodos de Aprendizaje Automático - InCo Definición • Características de este tipo de aprendizaje: ‣ No contamos con un conjunto de instancias <s, π(s)> ‣ No siempre se tiene una “recompensa” inmediata a una acción. ‣ No siempre una misma acción conduce a un mismo estado. ‣ Observación parcial de los estados. ‣ La política se aprende a medida que se la ejecuta. 5Métodos de Aprendizaje Automático - InCo Definición • Entonces: ‣ ¿Cómo se distribuye la recompensa en toda la cadena de acciones? ‣ Exploración vs. explotación: ¿cómo se balancea la búsqueda de nuevos datos con la obtención recompensas a partir de lo ya aprendido? ‣ Aprovechamiento de la información ya recolectada. 6Métodos de Aprendizaje Automático - InCo Definición • Escenarios posibles: ‣ ¿Son las acciones del agente deterministas? ‣ ¿Puede el agente predecir el resultado de su acción? ‣ ¿Se cuenta con un experto que enseñe? ‣ ¿Se puede elegir la secuencia de entrenamiento? • Consideremos procesos de decisión de Markov (MDPs): ‣ El tiempo es discreto. ‣ El agente selecciona una acción a t en estado s t ‣ El ambiente retorna una recompensa r t =r(s t ,a t ) y el siguiente estado s t+1 =δ(s t ,a t ) . El agente desconoce estas funciones. ‣ Nada depende de la secuencia de acciones previas al estado s t . 7Métodos de Aprendizaje Automático - InCo Definición • Buscamos una secuencia de acciones π que maximice el retorno acumulado con descuento: donde 0≤γ<1 pondera el retorno inmediato vs. el futuro • La estrategia óptima π* será aquella que maximiza el retorno: • Otras posibles V: • horizonte finito: • recompensa media: 8 G π*(s) G 00 r(s,a) 0 0 0 0 0 0 0 100 0 0 100 90 100 0 81 90 100 V*(s) con γ=0,9 : π*(s) = argmax π V π (s), ∀s V*(s) = V π* (s) V π (s t ) = r t + γr t+1 + γ 2 r t+2 + … = ∑ i γ i r t+i h ∑ i γ i r t+i lim h→∞ 1 h h ∑ i γ i r t+i Métodos de Aprendizaje Automático - InCo Aprendizaje Q • ¿Cómo obtenemos π*? ‣ Intentamos aprender V*. ‣ Luego, la acción a tomar es la que lleva al siguiente estado que maximiza la recompensa: ๏ Se requiere conocer r y δ. ๏ ¿Qué sucede cuando no se cuenta con información perfecta? 9 π*(s) = argmax a∈A [r(s, a) + γV*(δ(s, a))] Métodos de Aprendizaje Automático - InCo Aprendizaje Q • Utilizamos otra función de evaluación: • La acción a tomar es : • Esto es simplemente una reescritura, pero si logro aproximar Q, no preciso conocer directamente a r ni a δ. • Para aproximarla tomamos en cuenta la definición de V*: 10 90 100 0 81 90 100 V*(s) G 090 Q(s,a) 81 72 72 81 81 90 100 81 90 100 81 } Q(s, a) = r(s, a) + γV*(δ(s, a)) π*(s) = argmax a∈A Q(s, a) Q(s, a) = r(s, a) + γmax a 2 Q(δ(s, a), a 2 ) Q(s, a) = r(s, a) + γV*(δ(s, a)) V*(s) = max a Q(s, a) Métodos de Aprendizaje Automático - InCo Aprendizaje Q • Estimamos la función Q: ๏ Inicializo la tabla con valores nulos. ๏ Elijo una acción. ๏ Veo el resultado: r y s’. ๏ Actualizo Q(s,a). • Observar que en el ejemplo: ‣ En la primer corrida no se actualiza ninguna entrada hasta llegar a G. ‣ En cada corrida los valores se propagan hacia atrás a medida que se actualiza. 11 s 1 G 073 en t ̂ Q(s, a) 0 0 0 66 0 0 0 81 0 100 0 s 2 G 090 en t+1 ̂ Q(s, a) 0 0 0 66 0 0 0 81 0 100 0 derecha ← 0 + 0,9 max({66,81,100 } ) ← 90 ̂ Q(s 1 , derecha) ̂ Q(s 1 , derecha) ̂ Q(s, a) ← r(s, a) + γmax a 2 ̂ Q(δ(s, a), a 2 ) ̂ Q Métodos de Aprendizaje Automático - InCo Aprendizaje Q • Convergencia: Sea un MDP determinista con recompensa acotada. La tabla de se inicializa con valores aleatorios. Si el agente visita todo estado- acción infinitas veces, converge a Q. ̂ Q ̂ Q 12Métodos de Aprendizaje Automático - InCo Explotación vs. Exploración • Lo razonable sería elegir siempre la de mayor recompensa estimada. (explotación) • Pero se pueden perder otras mejores, además de no cumplir la visita “infinita” a todos los pares (s,a). (exploración) • Se puede establecer una política de exploración aleatoria, por ejemplo, ponderada por lo ya conocido... 13 P(a i |s) = k ̂ Q(s,a i ) ∑ j k ̂ Q(s,a j ) Métodos de Aprendizaje Automático - InCo ¿Cómo agilitar el aprendizaje? • Podemos repetir un mismo episodio (en memoria) las veces que queramos... • Actualizar la secuencia en orden inverso a su ejecución. • Recolectar los <s,a,r> y reentrenar periódicamente con estos valores. 14Métodos de Aprendizaje Automático - InCo Aprendizaje TD • El algoritmo Q es un caso particular de aprendizaje por diferencia temporal. • Estos algoritmos reducen la diferencia de estimación que hace un agente con el paso del tiempo: miro 1 paso delante miro 2 pasos ... miro n pasos • Todas esas fórmulas a su vez se pueden combinar en una: • Cuanto mayor es el valor de λ, más pesan los valores más alejados. 15 Q (n) (s t , a t ) ← r t + γr t+1 + γ 2 r t+2 + … + γ n−1 r t+n−1 + γ n max a ̂ Q(s t+n , a) Q (2) (s t , a t ) ← r t + γr t+1 + γ 2 max a ̂ Q(s t+2 , a) Q (1) (s t , a t ) ← r t + γmax a ̂ Q(s t+1 , a) Q λ (s t , a t ) = (1 − λ) . [Q (1) (s t , a t ) + λQ (2) (s t , a t ) + λ 2 Q (3) (s t , a t ) + …] Q λ (s t , a t ) ← r t + γ[(1 − λ)max a ̂ Q(s t+1 , a) + λQ λ (s t+1 , a t+1 )] Métodos de Aprendizaje Automático - InCo Ambiente no-determinista • ¿Qué sucede cuando acciones y recompensas no son deterministas? • Generalizamos el algoritmo de aprendizaje Q: • Como r no es determinista, la anterior regla de aprendizaje no converge; utilizamos entonces la siguiente regla: donde α n actúa como tasa de aprendizaje, por ejemplo: 16 V π (s t ) = E( ∑ i γ i r t+i ) Q(s, a) = E(r(s, a) + γV*(δ(s, a))) = E(r(s, a)) + γE(V*(δ(s, a))) = E(r(s, a)) + γ ∑ s′ P(s′ |s, a)V*(s′ ) = E(r(s, a)) + γ ∑ s′ P(s′ |s, a) . max a′ Q(s′ , a′ ) ̂ Q(s, a) ← (1 − α n ) . ̂ Q(s, a) + α n [r(s, a) + γmax a 2 ̂ Q(δ(s, a), a 2 )] α n = 1 visitas(s, a) Métodos de Aprendizaje Automático - InCo Ambiente no-determinista • Convergencia: (H) Sea un MDP no determinista con recompensa acotada, y n(i,s,a) la iteración correspondiente a la i-ésima vez que la acción a se aplica en s. Si todo par estado-acción se visita infinitas veces, y: (T) converge a Q ̂ Q 17 ∞ ∑ i=1 α n (i, s, a) = ∞ ∞ ∑ i=1 α n (i, s, a) 2 < ∞ 0 ≤ α n < 1 Métodos de Aprendizaje Automático - InCo Representación de Q • No siempre se puede tener una tabla para representar Q. • Además, se puede intentar generalizar Q a partir de los ejemplos vistos. • La función, entonces, se puede representar con una función lineal, una red neuronal, etc. • Problema: la convergencia de a Q no está garantizada. ̂ Q 18Métodos de Aprendizaje Automático - InCo Representación de Q 19Métodos de Aprendizaje Automático - InCo Representación de Q 20Métodos de Aprendizaje Automático - InCo Refuerzos y una vida de juegos :) • 1963 - Ta-Te-Ti (Menace) • 1992 - Backgammon (TD-Backgammon) • 2016-2017 - Go (AlphaGo - AlphaGo Zero) • 2018 - Ajedrez, Shogi y Go (AlphaZero) • 2019 - Atari* (MuZero) 21","libVersion":"0.3.1","langs":""}