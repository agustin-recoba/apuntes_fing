{"cells":[{"cell_type":"markdown","metadata":{"id":"uNKdOYpcOYyG"},"source":["# Regresión Logística\n","### Aprendizaje Automático - Instituto de Computación - UdelaR\n"]},{"cell_type":"markdown","metadata":{"id":"qEMDdo7VOYyI"},"source":["## Regresión Logística\n","\n","La regresión logística es un método de clasificación (igual que, por ejemplo, los árboles de decisión o los clasificadores bayesianos). La especificación es similar al caso de la regresión lineal, pero en este caso $y$, en lugar de tomar valores continuos, toma valores discretos. Empecemos por suponer que $y$ vale $0$ o $1$ (clasificación binaria), y luego veremos como generalizarlo.\n","\n","La regresión logística no solamente intenta predecir la clase $y$, sino que lo hace intentando estimar la probabilidad de que valga 1 (y, al mismo tiempo, la de que valga 0). La regresión logística es un *clasificador probabilístico*.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dhnXpY14OYyM"},"source":["## ¿Por qué no utilizar simplemente regresión lineal, suponiendo que $y$ es continua (aunque sepamos que solamente toma valores 0 y 1)?"]},{"cell_type":"markdown","metadata":{"id":"LoblcugjRyti"},"source":["<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*mM_qElOkQIZ3eR6nwc9yng.png\" width=400px/> <br>\n","<img src=\"https://miro.medium.com/v2/1*Kdth3dTZRTeeg0QY3l1P_w.jpeg\" width=400px/> <br>\n","# Outliers<br>\n","<img src=\"https://miro.medium.com/v2/1*57QgHJNelV829yj4XoD8RQ.png\" width=700px/> <br>\n","<img src=\"https://miro.medium.com/v2/1*EVzm1iP8tx0Opvw-ph-XrA.png\" width=700px/>"]},{"cell_type":"markdown","metadata":{"id":"cDGjlbQCOYyJ"},"source":["## Regresión Logística\n","\n","\n","Podemos identificar cuatro componentes principales de un clasificador que utilice aprendizaje automático:\n","\n","1. Una representación de los atributos de la entrada $[x_1, x_2, \\ldots, x_n]$, para cada observación $x^{(i)}$\n","2. Una función de hipótesis que calcula $\\hat{y}$, la clase predicha. Veremos aquí una de ellas: la función *sigmoide*\n","3. Una función objetivo (usualmente llamada función de costo) para el aprendizaje, que generalmente implica minimizar un error sobre los ejemplos de entrenamiento. Ya vimos una función así en la regresión lineal, y presentaremos dos más vinculadas a la regresión logística\n","4. Un algoritmo para optimizar la función objetivo. Ya vimos descenso por gradiente, y volveremos a utilizarlo aquí"]},{"cell_type":"markdown","metadata":{"id":"wcPp4D7nOYyK"},"source":["## La función sigmoide\n","\n","Para intentar calcular la probabilidad $P(y=1 | x;\\theta)$ de que la clase sea 1, dada la instancia y con nuestros parámetros,  empezaremos por calcular la combinación lineal de los parámetros\n","\n","$$z= \\theta_0 + x^T\\theta$$\n","\n","Como $z$ no toma valores entre 0 y 1, lo pasaremos por una función sigmoide:\n","\n","$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n","\n","Y, con ello, obtendremos las probabilidades que buscamos:\n","\n","$$ P(y=1) = \\sigma(\\theta_0 + x^T\\theta) $$\n","\n","$$ P(y=0) = 1 - P(y=1) $$\n"]},{"cell_type":"markdown","metadata":{"id":"P0ydbkMHOYyK"},"source":["## La función sigmoide\n","\n","Para clasificar una instancia, simplemente agregamos una _frontera de decisión_:\n","\n"," \\begin{equation}\n","    \\hat{y}=\n","    \\begin{cases}\n","      1 & \\text{if } P(y=1|x) > 0.5\\\\\n","      0 & \\text{en otro caso}\n","    \\end{cases}\n","  \\end{equation}\n","\n","\n","(Al igual que en el caso de la regresión lineal, la frontera de decisión no tiene por qué ser lineal respecto a las variables de entrada: basta con considerar atributos que son combinaciones de variables de entrada para obtener hipótesis polinomiales, o más complejas, respecto a las variables de entrada)"]},{"cell_type":"markdown","metadata":{"id":"M03dKVxTOYyK"},"source":["<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*dZ-9rn0vG7fZacc33k8Zwg.jpeg\" style=\"width: 450px;\"/>  <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*WrjkswcLc0oqxIN5rNT2dg.png\" style=\"width: 450px;\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"w1IrypoROYyL"},"source":["## La función sigmoide\n","\n","$g(z)$ es conocida como la _función logística_ o _sigmoide_, y luce así:\n","\n","<img src=\"https://miro.medium.com/max/1400/1*6A3A_rt4YmumHusvTvVTxw.png\" alt=\"Sigmoide\" style=\"width: 450px;\"/>\n","\n","Comentarios:\n","\n","- puede verse como una función de probabilidad continua, ya que tiende a $0$ cuando $z$ tiende a $-\\infty$, y tiende a $1$ cuando $z$ tiende a $+\\infty$.\n","\n","- es diferenciable, con una derivada muy simpática: $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$\n","\n","- tiene un punto de inflexión en 0.5\n","\n","- como tiene esas \"mesetas\" cuando se acerca a 0 o 1, tiende a llevar a los outliers a esos extremos"]},{"cell_type":"markdown","metadata":{"id":"gChdjYSVOYyM"},"source":["### Ejemplo (Martin & Jurafsky)\n","\n","Supongamos que queremos evaluar si un documento expresa una opinión positiva (1) o negativa (0), y lo representamos así:\n","\n","| Atributo        | Descripción |\n","| ------------- |:-------------|\n","| x_1 | count(lexicon positivos) |\n","| x_2 | count(lexicon negativos) |\n","| x_3 | 1 si aparece la palabra \"no\" |\n","| x_4 | count(pronombres de primera y segunda persona)|\n","| x_5 | 1 si aparece \"!\"\n","| x_6 | log(cantidad de palabras) |\n","\n","\n","y tenemos $\\theta=[2.5, -5.0, -1.2, 0.5, 2.0, 0.7]$ y $\\theta_0=0.1$.\n","\n","Obsérvese que $\\theta_1$ es positivo, lo que indica que el atributo contribuye a una una opinión positiva, mientras que el segundo y el tercero, por el contrario contribuyen a una negativa. Además, podemos ver (por sus valores absolutos), que el segundo atributo es el doble más importante que el primero en su contribución.\n","\n","Calculamos las probabilidades de una instancia $x$ con valores $[3, 2,1,3,0, ln(66)=4.19]$\n","\n","$$P(+| x) = \\sigma(0.1+[2.5, -5.0, -1.2, 0.5, 2.0, 0.7] \\cdot [3,2,1,3,0,4.19] = \\sigma(0.833) = 0.70$$\n","$$P(-| x) = 0.30$$\n"]},{"cell_type":"markdown","metadata":{"id":"htlRoZmbOYyN"},"source":["## Función de costo para la regresión logística\n","\n","La función $J(\\theta)$ utilizada para la regresión lineal no es adecuada, ya que con la introducción de $g(z)$ en la hipótesis, deja de ser convexa, y por lo tanto presenta varios mínimos locales que impedirían el descenso por gradiente. Por lo tanto, construiremos una nueva función $J(\\theta)$, convexa, que luego intentaremos minimizar.\n","\n","Recordemos que $J(\\theta)$ mide qué tanto difiere la predicción $\\hat{y}=h_\\theta(x)$ del verdadero $y$. Buscaremos una función que maximice la probabilidad de los valores de la clase en el corpus de entrenamiento (estimador de máxima verosimilitud condicional). Para ser más exactos, su logaritmo (lo cual es equivalente).\n","\n","$$ p(y|x) = \\hat{y}^y (1-\\hat{y})^{1-y}$$\n","\n","(aunque la fórmula parece horrible, observe qué devuelve en cada combinación de valores de $y$ y $\\hat{y}$)\n","\n","$$ \\log p(y|x) = y\\log\\hat{y} + (1-y)\\log(1-\\hat{y}) $$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ijh6PCqmOYyN"},"source":["## Función de costo para la regresión logística\n","\n","\n","Como lo que queremos es una función a minimizar... cambiamos el signo y, con ello, obtenemos la función de costo, correspondiente a minimizar el negativo del logaritmo del estimador de máxima verosimilitud... también llamada *función de entropía cruzada* (*cross-entropy loss function*)\n","\n","$$L_{CE}(\\hat{y},y) = -\\log p(y|x) $$\n","\n","y, por lo tanto, nuestra función de costo sobre todos los ejemplos será:\n","\n","$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m L_{CE}(h_\\theta(x^{(i)}),y^{(i)})$$\n","\n","Esta función de costo así construida tiene algunas propiedades interesantes: vale $0$ si el valor predicho por la hipótesis es igual a $y$, y tiende a infinito si difiere. Por lo tanto, está penalizando el error en la predicción de los valores de entrenamiento. Además, resulta ser convexa, por lo que es posible obtener el mínimo global utilizando métodos iterativos, como el descenso por gradiente\n","\n","\n","$$\n","J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\left [ y^{(i)} log (h_\\theta(x^{(i)})) + (1 - y^{(i)} ) log (1 - h_\\theta(x^{(i)}))\\right ]\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"EDDUunhmOYyN"},"source":["## Descenso por gradiente para regresión logística\n","\n","Utilizando descenso por gradiente igual que hicimos con la regresión lineal:\n","\n","$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) $$\n","\n","Para nuestra elección de $J(\\theta)$ (es decir, utilizando entropía cruzada), la regla de actualización es:  \n","\n","$$ \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j  $$\n","\n","$$ \\text{(actualizando todos los } \\theta_j \\text{ a la vez)} $$\n","\n","o, en su versión vectorial:\n","\n","$$ \\theta := \\theta - \\alpha \\frac{1}{m} X^T (g(X\\theta) - y)  $$"]},{"cell_type":"markdown","metadata":{"id":"KgdY-wD-OYyN"},"source":["## Regularización\n","\n","Para evitar el sobreajuste, e igual que hicimos con la regresión lineal, modificamos la función de costo para penalizar los valores muy grandes de los parámetros:\n","\n","$$J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)}\\ \\log (h_\\theta (x^{(i)})) + (1 - y^{(i)})\\ \\log (1 - h_\\theta(x^{(i)}))\\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2$$\n","\n","Y obtenemos nuestra nueva regla de actualización para descenso por gradiente:\n","\n","$$ \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_0 $$\n","\n","$$\\theta_j := \\theta_j - \\alpha \\left [ \\left ( \\frac{1}{m}\\sum_{i=1}^{m}  (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j \\right\n",") + \\frac{\\lambda}{m}\\theta_j  \\right ]  $$"]},{"cell_type":"markdown","metadata":{"id":"XAewOAB8OYyO"},"source":["## Regresión logística multiclase (1-versus-rest)\n","\n","Una vez resuelto el problema de la regresión logística para clasificación binaria, lo extenderemos al caso en que $y \\in \\{0,1,2 \\ldots n\\}$. Para ello dividiremos nuestro problema en $(n+1)$ problemas de clasificación binaria, donde en cada uno obtendremos un valor de $h_\\theta^{(i)}$ para el problema de separar las instancias de la clase $i$ del resto. Luego, dado un ejemplo $x$, predeciremos la clase $i$ para la que el valor de $h_\\theta^{(i)}(x)$ es máximo:\n","\n","$$\n","\\begin{align}\n","y \\in \\{1,2\\ldots n\\}\\\\\n","h_\\theta^{(i)} = P(y=i\\ \\mid x;\\theta) \\\\\n","\\text{predecir  para } x = \\max_i (h_\\theta^{(i)})\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"7LuAr9VgOYyO"},"source":["<img src=\"https://miro.medium.com/max/1400/1*RElrybCZ4WPsUfRwDl7fqA.png\" />"]},{"cell_type":"markdown","metadata":{"id":"8Tja8VAVOYyO"},"source":["## Siguientes clases: Redes neuronales\n"]},{"cell_type":"markdown","metadata":{"id":"vePf_H6MOYyO"},"source":["## Referencias\n","\n","- [Logistic Regression](https://web.stanford.edu/~jurafsky/slp3/5.pdf) - Capítulo 5 (draft) de la 3era edición del libro \"Speech and Language Processing\" de Martin and Jurafsky.\n","- [Notas del curso CS229](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) de la Universidad de Stanford (disponible en la plataforma Coursera)"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
